{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score)\n",
    "from sklearn.model_selection import learning_curve,GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing standard packages\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy import stats \n",
    "\n",
    "# importing the plot funnctions\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# preprocessing/ model selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# importing the classifiers \n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# importing the metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report,f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# oversampling techniques \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# importing model saving package \n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling_unbalanced(train_x, train_y, sample_method):\n",
    "    \"\"\" \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_x : test_x \n",
    "            pd.DataFrame\n",
    "    train_y : train_y \n",
    "            pd.DataFrame\n",
    "    sample_method: 'over' or 'under'\n",
    "            string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_x : resampled train_x\n",
    "                pd.DataFrame\n",
    "    train_y : resampled train_y\n",
    "                pd.DataFrame\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if sample_method == \"over\":\n",
    "        oversample = SMOTE()\n",
    "        train_x_ros, train_y_ros = oversample.fit_resample(X_train, y_train)\n",
    "        \n",
    "        return train_x_ros, train_y_ros\n",
    "        \n",
    "    if sample_method == \"under\":\n",
    "        # Concate X and Y train\n",
    "        trainData = pd.concat([train_x, train_y],axis=1)\n",
    "        # Class count\n",
    "        count_class_0, count_class_1 = trainData[\"hospital_death\"].value_counts()\n",
    "\n",
    "        # Divide by class\n",
    "        df_class_0 = trainData[trainData['hospital_death'] == 0]\n",
    "        df_class_1 = trainData[trainData['hospital_death'] == 1]\n",
    "        \n",
    "        # Sample the majority class\n",
    "        df_class_0_under = df_class_0.sample(count_class_1)\n",
    "        \n",
    "        # Put the train dataset together\n",
    "        train_rus = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "        train_x_rus = train_rus.drop(\"hospital_death\", axis = 1)\n",
    "        train_y_rus = pd.DataFrame(train_rus['hospital_death'])\n",
    "        \n",
    "        return train_x_rus, train_y_rus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the stored data frames\n",
    "%store -r X_wids\n",
    "%store -r y_wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((91713, 598), (91713, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wids.shape, y_wids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73370, 598) (18343, 598) (73370, 1) (18343, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split train-test dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wids, y_wids, test_size = 0.2, random_state = 31, stratify = y_wids)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_us = resampling_unbalanced(X_train, y_train, 'under')[0]\n",
    "y_train_us = resampling_unbalanced(X_train, y_train, 'under')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters without tuning\n",
    "\n",
    "params = {'bosting_type' :'gbdt',\n",
    "          'max_depth' : 7,\n",
    "          'objective': 'binary',\n",
    "          'learning_rate': 0.1,\n",
    "          'n_estimators': 100}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create parameters to search\n",
    "\n",
    "gridParams = {\n",
    "    'learning_rate': [0.1,0.05,0.01],\n",
    "    'n_estimators': [100,150,200,250],\n",
    "    'max_depth': [7, -1, 10],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier to use\n",
    "\n",
    "md_1 = LGBMClassifier(boosting_type= 'gbdt',\n",
    "          objective = 'binary',\n",
    "          max_depth = params['max_depth'],\n",
    "          n_estimators = params['n_estimators'],\n",
    "          learning_rate = params['learning_rate'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['boosting_type', 'class_weight', 'colsample_bytree', 'importance_type', 'learning_rate', 'max_depth', 'min_child_samples', 'min_child_weight', 'min_split_gain', 'n_estimators', 'n_jobs', 'num_leaves', 'objective', 'random_state', 'reg_alpha', 'reg_lambda', 'silent', 'subsample', 'subsample_for_bin', 'subsample_freq'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To view the default model params:\n",
    "md_1.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid\n",
    "\n",
    "grid = GridSearchCV(md_1, gridParams,\n",
    "                    verbose=0,\n",
    "                    cv=3,\n",
    "                    n_jobs=1,\n",
    "                   scoring = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'gbdt', 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'objective': 'binary'}\n",
      "0.8128678488536535\n"
     ]
    }
   ],
   "source": [
    "# Run the grid with under-sampled data\n",
    "grid.fit(X_train_us, y_train_us)\n",
    "\n",
    "\n",
    "# Print the best parameters found\n",
    "\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with params: \n",
      "{'bosting_type': 'gbdt', 'max_depth': 7, 'objective': 'binary', 'learning_rate': 0.05, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Using parameters already set above, replace in the best from the grid search\n",
    "\n",
    "params['max_depth'] = grid.best_params_['max_depth']\n",
    "params['learning_rate'] = grid.best_params_['learning_rate']\n",
    "params['n_estimators'] = grid.best_params_['n_estimators']\n",
    "\n",
    "\n",
    "\n",
    "print('Fitting with params: ')\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of classifers with the hyperparameters you would like to try \n",
    "classifiers = {'LGB_U_tuned': LGBMClassifier(boosting_type= 'gbdt',\n",
    "          objective = 'binary',\n",
    "          max_depth = params['max_depth'],\n",
    "          n_estimators = params['n_estimators'],\n",
    "          learning_rate = params['learning_rate'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys for the classifiers \n",
    "selected_clfs = ['LGB_U_tuned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- classifier being run is LGB_U_tuned ---------\n",
      "-------- saving the the model ---------\n",
      "-------- computing y_predict, y_prob, y_train_predict ---------\n",
      "-------- creating the confusion matrix ---------\n",
      "-------- calculating the metrics ---------\n",
      "Cross Validation is on 4 folds.\n",
      "-------- append values to the model matrix  ---------\n"
     ]
    }
   ],
   "source": [
    "clf_under_matrix = pd.DataFrame(columns = ['model', 'precision_test', 'recall_test', 'FPR_test', 'AUROC_test',\n",
    "                                     'f1_test', 'f1_train', 'f1_CV'])\n",
    "\n",
    "# for every classifer in the selected classifers list\n",
    "for idx, classifier in enumerate(selected_clfs):\n",
    "\n",
    "    # get the classifer and hyperparameters from the model\n",
    "    clf = classifiers[classifier]\n",
    "\n",
    "    # fit the model \n",
    "    clf.fit(X_train_us, y_train_us.values.ravel())\n",
    "    print(f\"-------- classifier being run is {classifier} ---------\")\n",
    "\n",
    "    print(f\"-------- saving the the model ---------\")\n",
    "    # Write the model to file\n",
    "    dump(clf, '{}.joblib'.format(classifier)) \n",
    "\n",
    "    print(f\"-------- computing y_predict, y_prob, y_train_predict ---------\")\n",
    "    # prediction of y based on X_test\n",
    "    y_predict = clf.predict(X_test)\n",
    "    # prediction of y probability based on X_test\n",
    "    y_proba = clf.predict_proba(X_test)[:,1]\n",
    "    # prediction of y based on X_train\n",
    "    y_train_predict = clf.predict(X_train_us)\n",
    "\n",
    "    print(f\"-------- creating the confusion matrix ---------\") \n",
    "    # confusion matrix \n",
    "    cmat = confusion_matrix(y_test.values.ravel(), y_predict)\n",
    "    # tp, fn, fp, tn\n",
    "    tp = cmat[1,1]\n",
    "    fn = cmat[1,0]\n",
    "    fp = cmat[0,1]\n",
    "    tn = cmat[0,0]\n",
    "\n",
    "    print(f\"-------- calculating the metrics ---------\") \n",
    "    #precision score on test\n",
    "    p_score_test = precision_score(y_test.values.ravel(), y_predict)\n",
    "\n",
    "    #recall score on test\n",
    "    r_score_test = tp/(tp+fn) \n",
    "\n",
    "    #FPR score on test\n",
    "    fpr_test = fp/(fp+tn)\n",
    "\n",
    "    #AUROC score on test\n",
    "    auroc_test = roc_auc_score(y_test.values.ravel(), y_proba)\n",
    "\n",
    "    #f1 score on test\n",
    "    f1_test = f1_score(y_test.values.ravel(), y_predict)\n",
    "\n",
    "    #f1 score on train\n",
    "    f1_train = f1_score(y_train_us.values.ravel(), y_train_predict)\n",
    "\n",
    "    #f1 score on cross validation \n",
    "    k_fold = 4\n",
    "    f1_cv = cross_val_score(clf, X_train_us, y_train_us.values.ravel(), cv=k_fold, scoring = 'f1')\n",
    "    print(f\"Cross Validation is on {k_fold} folds.\")\n",
    "\n",
    "    print(f\"-------- append values to the model matrix  ---------\") \n",
    "\n",
    "    # append to matrix\n",
    "    df2 = pd.DataFrame([[classifier,p_score_test,r_score_test, fpr_test, auroc_test, f1_test, f1_train, f1_cv]], \n",
    "                       columns=['model', 'precision_test', 'recall_test', 'FPR_test', 'AUROC_test',\n",
    "                                     'f1_test', 'f1_train', 'f1_CV'])\n",
    "    \n",
    "    clf_under_matrix = pd.concat([df2, clf_under_matrix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>FPR_test</th>\n",
       "      <th>AUROC_test</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LGB_U_tuned</td>\n",
       "      <td>0.291713</td>\n",
       "      <td>0.820594</td>\n",
       "      <td>0.188186</td>\n",
       "      <td>0.8997</td>\n",
       "      <td>0.430417</td>\n",
       "      <td>0.912183</td>\n",
       "      <td>[0.8044657097288676, 0.816822429906542, 0.8191...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  precision_test  recall_test  FPR_test  AUROC_test   f1_test  \\\n",
       "0  LGB_U_tuned        0.291713     0.820594  0.188186      0.8997  0.430417   \n",
       "\n",
       "   f1_train                                              f1_CV  \n",
       "0  0.912183  [0.8044657097288676, 0.816822429906542, 0.8191...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_under_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
