{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NsyVL27dBlWI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivianahernandez/anaconda3/envs/uchi/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# importing standard packages\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy import stats \n",
    "\n",
    "# importing the plot funnctions\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# preprocessing/ model selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# importing the classifiers \n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# importing the metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report,f1_score\n",
    "\n",
    "# importing model saving package \n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rH25auEUCxb3"
   },
   "outputs": [],
   "source": [
    "def draw_conf_mat(mat):\n",
    "    \n",
    "    \"\"\"\n",
    "    Draw confusion matrix\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    mat:                ndarray of shape (n_classes, n_classes) \n",
    "                        confusion matrix\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt \n",
    "    import seaborn as sns\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(mat, annot=True, fmt=\"d\",\n",
    "            xticklabels=['0', '1'],\n",
    "            yticklabels=['0', '1'])\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPZbfv6KCxgE"
   },
   "outputs": [],
   "source": [
    "def draw_roc_curve(y_true, y_proba):\n",
    "    \n",
    "    \"\"\"\n",
    "    Draw baseline and model roc curve\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    y_true:             array-like of shape (n_samples,) \n",
    "                        True label of target (y)\n",
    "                        \n",
    "    y_proba             array-like of shape (n_samples,)\n",
    "                        The predicted probability of target (y)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve\n",
    "    \n",
    "    base_fpr, base_tpr, _ = roc_curve(y_true, [1 for _ in range(len(y_true))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(y_true, y_proba)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "    \n",
    "    return axdef draw_roc_curve(y_true, y_proba):\n",
    "    \n",
    "    \"\"\"\n",
    "    Draw baseline and model roc curve\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    y_true:             array-like of shape (n_samples,) \n",
    "                        True label of target (y)\n",
    "                        \n",
    "    y_proba             array-like of shape (n_samples,)\n",
    "                        The predicted probability of target (y)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve\n",
    "    \n",
    "    base_fpr, base_tpr, _ = roc_curve(y_true, [1 for _ in range(len(y_true))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(y_true, y_proba)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_eWOeVWsCxjc"
   },
   "outputs": [],
   "source": [
    "def metrics_imbalanced(your_confusion_matrix, y_true, y_proba):\n",
    "    \n",
    "    \"\"\"\n",
    "    Print precision, recall, fallout and auroc score based on the confusion matrix. \n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    your_confusion_matrix:                ndarray of shape (n_classes, n_classes) \n",
    "                                          confusion matrix\n",
    "                                          \n",
    "    y_true:                               array-like of shape (n_samples,) \n",
    "                                          True label of target (y)\n",
    "                                          \n",
    "    y_proba:                              y_proba             array-like of shape (n_samples,)\n",
    "                                          The predicted probability of target (y)\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    # Model evaluation metrics. \n",
    "    tp = your_confusion_matrix[1,1]\n",
    "    fn = your_confusion_matrix[1,0]\n",
    "    fp = your_confusion_matrix[0,1]\n",
    "    tn = your_confusion_matrix[0,0]\n",
    "    auroc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    print('Precision = %0.3f'%(tp/(tp+fp)))\n",
    "    print('Recall (TPR) = %0.3f'%(tp/(tp+fn)))\n",
    "    print('Fallout (FPR) = %0.3f'%(fp/(fp+tn)))\n",
    "    print('Roc_auc_score = %0.3f'%auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CeWJxOh_FRXT"
   },
   "outputs": [],
   "source": [
    "def resampling_unbalanced(train_x, train_y, sample_method):\n",
    "    \"\"\" \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_x : test_x \n",
    "            pd.DataFrame\n",
    "    train_y : train_y \n",
    "            pd.DataFrame\n",
    "    sample_method: 'over' or 'under'\n",
    "            string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_x : resampled train_x\n",
    "                pd.DataFrame\n",
    "    train_y : resampled train_y\n",
    "                pd.DataFrame\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if sample_method == \"over\":\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        oversample = SMOTE()\n",
    "        train_x_ros, train_y_ros = oversample.fit_resample(X_train, y_train)\n",
    "        \n",
    "        return train_x_ros, train_y_ros\n",
    "        \n",
    "    if sample_method == \"under\":\n",
    "        # Concate X and Y train\n",
    "        trainData = pd.concat([train_x, train_y],axis=1)\n",
    "        # Class count\n",
    "        count_class_0, count_class_1 = trainData[\"hospital_death\"].value_counts()\n",
    "\n",
    "        # Divide by class\n",
    "        df_class_0 = trainData[trainData['hospital_death'] == 0]\n",
    "        df_class_1 = trainData[trainData['hospital_death'] == 1]\n",
    "        \n",
    "        # Sample the majority class\n",
    "        df_class_0_under = df_class_0.sample(count_class_1)\n",
    "        \n",
    "        # Put the train dataset together\n",
    "        train_rus = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "        train_x_rus = train_rus.drop(\"hospital_death\", axis = 1)\n",
    "        train_y_rus = pd.DataFrame(train_rus['hospital_death'])\n",
    "        \n",
    "        return train_x_rus, train_y_rus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in the X_wids and y_wids from the WIDS_Feature_Engineer.ipynb\n",
    "Run the WIDS Feature Engineer Notebook first in order to grab the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the stored data frames\n",
    "%store -r X_wids\n",
    "%store -r y_wids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split without Correction of Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IW5A_6luGi1J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73370, 453) (18343, 453) (73370, 1) (18343, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split train-test dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wids, y_wids, test_size = 0.2, random_state = 31, stratify = y_wids)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with No Correction Of Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Q7I7h51DZLh"
   },
   "outputs": [],
   "source": [
    "# create a dictionary of classifers with the hyperparameters you would like to try \n",
    "classifiers = {'XG': XGBClassifier(n_jobs = 1, learning_rate = .1, n_estimators = 100, objective = 'binary', booster='gbtree', max_depth = 7 ),\n",
    "              'RF': RandomForestClassifier(n_estimators= 100, max_depth = 7),\n",
    "              'LGB': LGBMClassifier(booster ='gbtree', n_estimators = 100, learning_rate = .1, max_depth = 7, objective ='binary')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0IQvBAsODZTj"
   },
   "outputs": [],
   "source": [
    "# keys of the classifiers \n",
    "selected_clfs = ['XG', 'RF', 'LGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmUV-PlKDZap"
   },
   "outputs": [],
   "source": [
    "# create the clf_matrix \n",
    "clf_matrix = pd.DataFrame(columns = ['model', 'precision_test', 'recall_test', 'FPR_test', 'AUROC_test',\n",
    "                                     'f1_test', 'f1_train', 'f1_CV'])\n",
    "\n",
    "# for every classifer in the selected classifers list\n",
    "for idx, classifier in enumerate(selected_clfs):\n",
    "\n",
    "    # get the classifer and hyperparameters from the model\n",
    "    clf = classifiers[classifier]\n",
    "\n",
    "    # fit the model \n",
    "    clf.fit(X_train,y_train)\n",
    "    print(f\"-------- classifier being run is {classifier} ---------\")\n",
    "\n",
    "    print(f\"-------- saving the the model ---------\")\n",
    "    # Write the model to file\n",
    "    dump(clf, '{}.joblib'.format(classifier)) \n",
    "\n",
    "    print(f\"-------- computing y_predict, y_prob, y_train_predict ---------\")\n",
    "    # prediction of y based on X_test\n",
    "    y_predict = clf.predict(X_test)\n",
    "    # prediction of y probability based on X_test\n",
    "    y_proba = clf.predict_proba(X_test)[:,1]\n",
    "    # prediction of y based on X_train\n",
    "    y_train_predict = clf.predict(X_train)\n",
    "\n",
    "    print(f\"-------- creating the confusion matrix ---------\") \n",
    "    # confusion matrix \n",
    "    cmat = confusion_matrix(y_test, y_predict)\n",
    "    # tp, fn, fp, tn\n",
    "    tp = cmat[1,1]\n",
    "    fn = cmat[1,0]\n",
    "    fp = cmat[0,1]\n",
    "    tn = cmat[0,0]\n",
    "\n",
    "    print(f\"-------- calculating the metrics ---------\") \n",
    "    #precision score on test\n",
    "    p_score_test = precision_score(y_test, y_predict)\n",
    "\n",
    "    #recall score on test\n",
    "    r_score_test = tp/(tp+fn) \n",
    "\n",
    "    #FPR score on test\n",
    "    fpr_test = fp/(fp+tn)\n",
    "\n",
    "    #AUROC score on test\n",
    "    auroc_test = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    #f1 score on test\n",
    "    f1_test = f1_score(y_test, y_predict)\n",
    "\n",
    "    #f1 score on train\n",
    "    f1_train = f1_score(y_train, y_train_predict)\n",
    "\n",
    "    #f1 score on cross validation \n",
    "    k_fold = 4\n",
    "    f1_cv = cross_val_score(clf, X_train, y_train, cv=k_fold, scoring = 'f1')\n",
    "    print(f\"Cross Validation is on {k_fold} folds.\")\n",
    "\n",
    "    print(f\"-------- append values to the model matrix  ---------\") \n",
    "\n",
    "    # append to matrix\n",
    "    df2 = pd.DataFrame([[classifier,p_score_test,r_score_test, fpr_test, auroc_test, f1_test, f1_train, f1_cv]], columns=['model', 'precision_test', 'recall_test', 'FPR_test', 'AUROC_test',\n",
    "                                     'f1_test', 'f1_train', 'f1_CV'])\n",
    "    \n",
    "    clf_matrix = pd.concat([df2, clf_matrix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKpE21Dod-y_"
   },
   "outputs": [],
   "source": [
    "# Draw confusion matrix for each model \n",
    "for classifier in selected_clfs:\n",
    "\n",
    "  # Load model saved\n",
    "  clf = load('{}.joblib'.format(classifier)) \n",
    "\n",
    "  # Predict y based on X_test\n",
    "  y_predict = clf.predict(X_test)\n",
    "\n",
    "  # confusion matrix\n",
    "  cm = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "  # Draw\n",
    "  print(classifier)\n",
    "  draw_conf_mat(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with Correction Of Imbalanced Data Using Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAQNTHxyCxqh"
   },
   "outputs": [],
   "source": [
    "# create a dictionary of classifers with the hyperparameters you would like to try \n",
    "classifiers = {'XG_U': XGBClassifier(n_jobs = 1, learning_rate = .1, n_estimators = 100, objective = 'binary', booster='gbtree', max_depth = 7 ),\n",
    "              'RF_U': RandomForestClassifier(n_estimators= 100, max_depth = 7),\n",
    "              'LGB_U': LGBMClassifier(booster ='gbtree', n_estimators = 100, learning_rate = .1, max_depth = 7, objective ='binary')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aeu5vtTECxmm"
   },
   "outputs": [],
   "source": [
    "# keys for the classifiers \n",
    "selected_clfs = ['XG_U', 'RF_U', 'LGB_U']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new train test split for imbalanced data. Undersampling Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = resampling_unbalanced(X_train, y_train, \"under\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "clf_under_matrix = pd.DataFrame(columns = ['model', 'precision_test', 'recall_test', 'FPR_test', 'AUROC_test',\n",
    "                                     'f1_test', 'f1_train', 'f1_CV'])\n",
    "\n",
    "# for every classifer in the selected classifers list\n",
    "for idx, classifier in enumerate(selected_clfs):\n",
    "\n",
    "    # get the classifer and hyperparameters from the model\n",
    "    clf = classifiers[classifier]\n",
    "\n",
    "    # fit the model \n",
    "    clf.fit(X_train,y_train)\n",
    "    print(f\"-------- classifier being run is {classifier} ---------\")\n",
    "\n",
    "    print(f\"-------- saving the the model ---------\")\n",
    "    # Write the model to file\n",
    "    dump(clf, '{}.joblib'.format(classifier)) \n",
    "\n",
    "    print(f\"-------- computing y_predict, y_prob, y_train_predict ---------\")\n",
    "    # prediction of y based on X_test\n",
    "    y_predict = clf.predict(X_test)\n",
    "    # prediction of y probability based on X_test\n",
    "    y_proba = clf.predict_proba(X_test)[:,1]\n",
    "    # prediction of y based on X_train\n",
    "    y_train_predict = clf.predict(X_train)\n",
    "\n",
    "    print(f\"-------- creating the confusion matrix ---------\") \n",
    "    # confusion matrix \n",
    "    cmat = confusion_matrix(y_test, y_predict)\n",
    "    # tp, fn, fp, tn\n",
    "    tp = cmat[1,1]\n",
    "    fn = cmat[1,0]\n",
    "    fp = cmat[0,1]\n",
    "    tn = cmat[0,0]\n",
    "\n",
    "    print(f\"-------- calculating the metrics ---------\") \n",
    "    #precision score on test\n",
    "    p_score_test = precision_score(y_test, y_predict)\n",
    "\n",
    "    #recall score on test\n",
    "    r_score_test = tp/(tp+fn) \n",
    "\n",
    "    #FPR score on test\n",
    "    fpr_test = fp/(fp+tn)\n",
    "\n",
    "    #AUROC score on test\n",
    "    auroc_test = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    #f1 score on test\n",
    "    f1_test = f1_score(y_test, y_predict)\n",
    "\n",
    "    #f1 score on train\n",
    "    f1_train = f1_score(y_train, y_train_predict)\n",
    "\n",
    "    #f1 score on cross validation \n",
    "    k_fold = 4\n",
    "    f1_cv = cross_val_score(clf, X_train, y_train, cv=k_fold, scoring = 'f1')\n",
    "    print(f\"Cross Validation is on {k_fold} folds.\")\n",
    "\n",
    "    print(f\"-------- append values to the model matrix  ---------\") \n",
    "\n",
    "    # append to matrix\n",
    "    df2 = pd.DataFrame([[classifier,p_score_test,r_score_test, fpr_test, auroc_test, f1_test, f1_train, f1_cv]], columns=['model', 'precision_test', 'recall_test', 'FPR_test', 'AUROC_test',\n",
    "                                     'f1_test', 'f1_train', 'f1_CV'])\n",
    "    \n",
    "    clf_under_matrix = pd.concat([df2, clf_under_matrix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw confusion matrix for each model \n",
    "for classifier in selected_clfs:\n",
    "\n",
    "  # Load model saved\n",
    "  clf = load('{}.joblib'.format(classifier)) \n",
    "\n",
    "  # Predict y based on X_test\n",
    "  y_predict = clf.predict(X_test)\n",
    "\n",
    "  # confusion matrix\n",
    "  cm = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "  # Draw\n",
    "  print(classifier)\n",
    "  draw_conf_mat(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGQ9M93zCxz3"
   },
   "source": [
    "## Models with Correction Of Imbalanced Data Using Oversampling SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PrZrc_zgCx6t"
   },
   "outputs": [],
   "source": [
    "# create a dictionary of classifers with the hyperparameters you would like to try \n",
    "classifiers = {'XG_O': XGBClassifier(n_jobs = 1, learning_rate = .1, n_estimators = 100, objective = 'binary', booster='gbtree', max_depth = 7 ),\n",
    "              'RF_O': RandomForestClassifier(n_estimators= 100, max_depth = 7),\n",
    "              'LGB_O': LGBMClassifier(booster ='gbtree', n_estimators = 100, learning_rate = .1, max_depth = 7, objective ='binary')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iF9wdNYVCx3T"
   },
   "outputs": [],
   "source": [
    "select_clfs = ['XG_O', 'RF_O', 'LGB_O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = resampling_unbalanced(X_train, y_train, \"over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJmp0nhGCx-T"
   },
   "outputs": [],
   "source": [
    "clf_over_matrix = pd.DataFrame(columns = ['model', 'precision_test', 'recall_test', 'FPR_test', 'AUROC_test',\n",
    "                                     'f1_test', 'f1_train', 'f1_CV'])\n",
    "\n",
    "# for every classifer in the selected classifers list\n",
    "for idx, classifier in enumerate(selected_clfs):\n",
    "\n",
    "    # get the classifer and hyperparameters from the model\n",
    "    clf = classifiers[classifier]\n",
    "\n",
    "    # fit the model \n",
    "    clf.fit(X_train,y_train)\n",
    "    print(f\"-------- classifier being run is {classifier} ---------\")\n",
    "\n",
    "    print(f\"-------- saving the the model ---------\")\n",
    "    # Write the model to file\n",
    "    dump(clf, '{}.joblib'.format(classifier)) \n",
    "\n",
    "    print(f\"-------- computing y_predict, y_prob, y_train_predict ---------\")\n",
    "    # prediction of y based on X_test\n",
    "    y_predict = clf.predict(X_test)\n",
    "    # prediction of y probability based on X_test\n",
    "    y_proba = clf.predict_proba(X_test)[:,1]\n",
    "    # prediction of y based on X_train\n",
    "    y_train_predict = clf.predict(X_train)\n",
    "\n",
    "    print(f\"-------- creating the confusion matrix ---------\") \n",
    "    # confusion matrix \n",
    "    cmat = confusion_matrix(y_test, y_predict)\n",
    "    # tp, fn, fp, tn\n",
    "    tp = cmat[1,1]\n",
    "    fn = cmat[1,0]\n",
    "    fp = cmat[0,1]\n",
    "    tn = cmat[0,0]\n",
    "\n",
    "    print(f\"-------- calculating the metrics ---------\") \n",
    "    #precision score on test\n",
    "    p_score_test = precision_score(y_test, y_predict)\n",
    "\n",
    "    #recall score on test\n",
    "    r_score_test = tp/(tp+fn) \n",
    "\n",
    "    #FPR score on test\n",
    "    fpr_test = fp/(fp+tn)\n",
    "\n",
    "    #AUROC score on test\n",
    "    auroc_test = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    #f1 score on test\n",
    "    f1_test = f1_score(y_test, y_predict)\n",
    "\n",
    "    #f1 score on train\n",
    "    f1_train = f1_score(y_train, y_train_predict)\n",
    "\n",
    "    #f1 score on cross validation \n",
    "    k_fold = 4\n",
    "    f1_cv = cross_val_score(clf, X_train, y_train, cv=k_fold, scoring = 'f1')\n",
    "    print(f\"Cross Validation is on {k_fold} folds.\")\n",
    "\n",
    "    print(f\"-------- append values to the model matrix  ---------\") \n",
    "\n",
    "    # append to matrix\n",
    "    df2 = pd.DataFrame([[classifier,p_score_test,r_score_test, fpr_test, auroc_test, f1_test, f1_train, f1_cv]], columns=['model', 'precision_test', 'recall_test', 'FPR_test', 'AUROC_test',\n",
    "                                     'f1_test', 'f1_train', 'f1_CV'])\n",
    "    \n",
    "    clf_over_matrix = pd.concat([df2, clf_over_matrix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xluRawugCyB2"
   },
   "outputs": [],
   "source": [
    "# Draw confusion matrix for each model \n",
    "for classifier in selected_clfs:\n",
    "\n",
    "  # Load model saved\n",
    "  clf = load('{}.joblib'.format(classifier)) \n",
    "\n",
    "  # Predict y based on X_test\n",
    "  y_predict = clf.predict(X_test)\n",
    "\n",
    "  # confusion matrix\n",
    "  cm = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "  # Draw\n",
    "  print(classifier)\n",
    "  draw_conf_mat(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZwOF1IzCyFL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Modeling_Iterations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
